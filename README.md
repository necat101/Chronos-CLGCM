# Chronos v0.6 (alpha): A Hybrid Memory-Reasoning Architecture

A novel AI architecture that synergistically integrates Google's Titans memory system with a Hierarchical Reasoning Model (HRM) to move beyond the limitations of scale and take a decisive step on the path to AGI.

-----

### 📢 **Major Update in v0.6: Efficient Large Dataset Handling**

> This version introduces significant improvements for training on **very large datasets** that may not fit into RAM, alongside the previous enhancements from v0.5.
>
> 1.  **Dataset Pre-processing Script (`dataset_chunk_create.py`):** A new standalone tool to pre-tokenize, chunk (with overlap), pad, and mask large JSONL datasets. It saves the processed chunks as individual, memory-efficient `.pt` tensor files.
> 2.  **Direct Tensor Loading (`--pre_pt_dataset`):** Chronos can now directly load and train on the `.pt` files generated by the chunking script. This **dramatically reduces RAM usage** during training startup and allows working with datasets far larger than available memory.
> 3.  **Iterable Dataset Support (`--pre_chunked_dataset`):** An alternative option to load pre-processed data from a large JSONL file line-by-line, providing another low-memory training strategy.

## About The Project

The field of AI has been dominated by a paradigm of unprecedented scale, yet fundamental limitations in today's Transformer models are becoming apparent. The path to Artificial General Intelligence (AGI) may not be paved with scale alone. Chronos challenges this paradigm by focusing on **architectural intelligence**.

This project introduces a novel hybrid model where a deep reasoning engine operates within a dynamic, lifelong learning memory environment. Chronos is conceived not merely to process information, but to **think, learn, and remember** in a cohesive, integrated, and human-like manner.

## Core Concepts

Chronos is built on two revolutionary, brain-inspired pillars:

🧠 **Titans Architecture (The Cognitive Substrate)**
A sophisticated, multi-tiered memory workspace that enables dynamic, lifelong learning. It learns *what to remember* based on the principle of "surprise," and its memory slots are now structured with timestamps and source metadata, allowing for sophisticated, context-aware queries.

⚙️ **Hierarchical Reasoning Model (The Cognitive Process)**
A powerful, data-efficient, and deep reasoning engine. Its dual-module design (a high-level "CEO" and low-level "Workers") allows for profound computational depth, enabling it to solve complex, multi-step algorithmic problems where massive LLMs fail.

## Features

  - 💾 **Efficient Pre-Chunked Dataset Loading**: Dramatically reduces RAM usage and speeds up training startup for large datasets using pre-processed `.pt` tensor files (`--pre_pt_dataset`).
  - 📜 **Iterable Dataset Support**: Option to load pre-chunked JSONL datasets line-by-line (`--pre_chunked_dataset`) for minimal memory overhead during training.
  - ✂️ **Dataset Pre-processing Script (`dataset_chunk_create.py`)**: New tool to prepare large datasets for efficient loading, handling tokenization, chunking (with intelligent anchoring for Kayla format), padding, and masking.
  - 🤔 **Adaptive "Ponder" Time**: Dynamically adjusts its reasoning depth, "thinking" longer for complex problems and saving computation on simpler ones.
  - 🕰️ **Structured & Queryable Memory**: LTM slots are augmented with timestamps and source data, enabling powerful temporal and contextual queries during chat.
  - 🧠 **Dynamic "Online" Learning**: Learns from experience during chat with a Cosine Annealing LR schedule by default for more stable knowledge consolidation.
  - ⚡ **Accelerated Training with AMP**: Supports Automatic Mixed Precision (`--amp`) for faster training and reduced memory usage on compatible NVIDIA GPUs.
  - 🛡️ **Stable Training**: Built-in gradient clipping to prevent model instability and ensure smoother convergence.
  - 📦 **Self-Contained & Portable Models**: Models are saved as directories containing weights, tokenizer, and architecture config for easy sharing and use.
  - 💾 **Automatic Re-quantization**: After a learning session, Chronos can automatically re-quantize a model to persist the new knowledge.
  - 🌱 **Enhanced Model Expansion**: Includes `expand_model.py` script to transplant weights from smaller models to larger ones, now supporting changes in `max_length` and automatic length detection from datasets.
  - ✨ **Flexible Training Initiation**: Supports starting training runs using weights from existing model directories (inference or expanded models), not just resuming full training checkpoints.
  - ⚡ **High-Performance Inference**: Utilizes a custom C++ kernel inspired by `llama.cpp` for state-of-the-art quantization (`INT4`, `Q4_0`, `Q8_0`, `Q2_K`).
  - 💻 **CPU & GPU Support**: Runs fast quantized inference on standard CPUs (with AVX) or on GPUs via Vulkan for broad hardware compatibility.
  - 🔧 **Comprehensive Tooling**: Includes a single script (`chronos.py`) for training, LoRA fine-tuning, merging, quantization, and interactive chat, plus the model expansion and dataset chunking scripts.

-----

## 🚀 Getting Started

Follow these steps to get a local copy up and running.

### Prerequisites

  - Python 3.8+
  - A C++ compiler (e.g., MSVC on Windows, GCC on Linux)
  - CMake (must be available in your system's `PATH`)
  - **For GPU Inference**: A Vulkan-compatible GPU and installed drivers.
  - **For GPU Training (AMP)**: An NVIDIA GPU with CUDA support (Compute Capability 7.0+ recommended for best performance) and a PyTorch build with CUDA enabled.
  - **For Developers (Re-compiling the Kernel)**: The official [Vulkan SDK](https://vulkan.lunarg.com/sdk/home) must be installed to compile the kernel with Vulkan support.

### Installation

1.  **Clone the repository:**

    ```bash
    git clone [https://github.com/your-username/Chronos.git](https://github.com/your-username/Chronos.git)
    cd Chronos
    ```

2.  **Create a virtual environment (recommended):**

    ```bash
    python -m venv .venv
    # On Windows
    .\.venv\Scripts\Activate
    # On Linux/macOS
    source .venv/bin/activate
    ```

3.  **Run the setup script:** This will install Python dependencies and compile the C++ inference kernel.

    ```bash
    # On Windows
    setup.bat
    # On Linux/macOS
    bash setup.sh
    ```

    This will create a `chronos_matmul` library file in your project root. If this fails, you can try running `pip install .` from the project root.

-----

## 📚 User Guide: Step-by-Step Workflows

The `chronos.py` script is the main entry point for model operations. The `dataset_chunk_create.py` script prepares large datasets, and `expand_model.py` handles weight transplantation. All models are handled as directories.

### **NEW**: Preparing Large Datasets (`dataset_chunk_create.py`) ✂️

If your dataset (`.jsonl` format, Kayla-style recommended) is too large to fit comfortably in RAM during training startup, use this script first. It analyzes the dataset, determines an optimal fixed sequence length, and then chunks each sample into multiple overlapping, padded `.pt` tensor files.

1.  **Run the Chunking Script:**

    ```bash
    python dataset_chunk_create.py \
        --dataset "path/to/your_large_data.jsonl" \
        --tokenizer-path "microsoft/phi-2" `# Or your chosen tokenizer` \
        --output-dir "./my_large_data_chunked_tensors" \
        --overlap 512 `# Adjust overlap as needed`
    ```

    * The script first analyzes the dataset to find the longest "thought-process" section (the anchor).
    * It then calculates the final `MAX_SEQ_LENGTH` based on this anchor, reserved space, and safety margins. **Note this `MAX_SEQ_LENGTH` value printed by the script.**
    * Finally, it processes the dataset, creating `.pt` files (e.g., `chunk_0000000.pt`, `chunk_0000001.pt`, ...) and a `manifest.jsonl` file in the `--output-dir`.

2.  **Use the Chunked Data for Training:**

    When running `chronos.py train` or `finetune`, use the `--pre_pt_dataset` flag, point `--train` to the *output directory* created by the chunker, and **specify the `MAX_SEQ_LENGTH`** noted from the chunker script's output using the `--max_length` argument.

    ```bash
    python chronos.py train \
        --pre_pt_dataset `# Enable loading from .pt files` \
        --train "./my_large_data_chunked_tensors" `# Directory containing .pt files` \
        --max_length 3153 `# MUST match the value printed by dataset_chunk_create.py` \
        --tokenizer-path "microsoft/phi-2" `# Still needed for model init if not resuming` \
        --out-dir "./my_large_chronos_model" \
        --epochs 3 \
        --batch_size 1 \
        --amp
    ```

    > **Important:**
    > * `--max_length` is **required** when using `--pre_pt_dataset` or `--pre_chunked_dataset`.
    > * The `--auto-max-length` and `--kayla` flags are **ignored** when using pre-chunked formats, as these aspects are handled by the `dataset_chunk_create.py` script.

### Basic Workflow (`chronos.py`)

1.  **Training (From Scratch - Standard Dataset):** Train a new model using a dataset that fits in RAM.

    ```bash
    python chronos.py train \
        --train "path/to/your_data.jsonl" \
        --tokenizer-path "microsoft/phi-2" \
        --out-dir "./my_chronos_model" \
        --epochs 5 \
        --batch_size 2 \
        --context_dim 512 \
        --max_h_steps 10 \
        --ponder-loss-weight 0.01 \
        --grad-clip 1.0 \
        --auto-max-length `# Optional: Scan dataset for max length` \
        --amp # <-- Optionally enable AMP
    ```

    -----

    💡 **Accelerating Training with AMP (NVIDIA GPUs):**
    If you are training on an NVIDIA GPU with CUDA support (Tensor Cores recommended, e.g., Volta, Turing, Ampere architecture or newer), you can enable **Automatic Mixed Precision (AMP)** using the `--amp` flag. AMP uses faster, lower-precision `float16` for many computations while maintaining model accuracy, significantly speeding up training and reducing VRAM usage. Make sure your PyTorch installation includes CUDA support.

    -----

2.  **Fine-Tuning (LoRA):** Adapt a pre-trained model using LoRA. Can use standard or pre-chunked datasets.

    ```bash
    # Using standard dataset
    python chronos.py finetune \
        --model-path "./my_chronos_model" \
        --train "path/to/new_data.jsonl" \
        --out-dir "./my_lora_adapter" \
        --epochs 3 \
        --amp

    # Using pre-chunked dataset
    python chronos.py finetune \
        --pre_pt_dataset \
        --model-path "./my_chronos_model" \
        --train "./my_finetune_data_chunked_tensors" \
        --max_length 3153 `# From chunker script` \
        --out-dir "./my_lora_adapter_large" \
        --epochs 1 \
        --amp
    ```

3.  **Merging a LoRA Adapter:** Merge the adapter back into the base model.

    ```bash
    python chronos.py merge-lora \
        --model-path "./my_chronos_model" \
        --lora-adapter-path "./my_lora_adapter" \
        --out-dir "./my_model_merged"
    ```

4.  **Quantization:** Convert a full-precision model directory into a quantized model directory.

    ```bash
    python chronos.py quantize \
        --model-path "./my_model_merged" \
        --out-dir "./my_model_merged-INT4" \
        --qtype INT4
    ```

    > **Available `qtype`:** `INT4`, `Q4_0`, `Q8_0`, `Q2_K`.

5.  **Inference (Chat Mode):** Run an interactive chat session.

    ```bash
    python chronos.py chat \
        --model-path "./my_model_merged-INT4"
    ```

### Resuming Incomplete Training (`chronos.py`)

Resume using a training checkpoint (`chronos_epoch_*.pt` file). The script will automatically detect the dataset format (`.jsonl` vs `.pt` directory) based on flags saved in the checkpoint if available, or you can specify them.

```bash
python chronos.py train \
    `# Specify dataset type ONLY if not correctly saved in checkpoint` \
    `# --pre_pt_dataset` \
    `# --train "./my_large_data_chunked_tensors" # Optional if path saved in ckpt` \
    `# --max_length 3153 # Optional if saved in ckpt` \
    --out-dir "./my_large_chronos_model" \
    --resume-from-ckpt "./my_large_chronos_model/chronos_epoch_1.pt" \
    --epochs 3 `# Set total desired epochs` \
    --amp # <-- Remember AMP if resuming an AMP session
````

#### Resuming Training with Modified Learning Rate

Use `--override-scheduling` along with new `--starting-lr` and `--min-lr` values.

```bash
python chronos.py train \
    --out-dir "./my_large_chronos_model" \
    --resume-from-ckpt "./my_large_chronos_model/chronos_epoch_2.pt" \
    --epochs 3 \
    --starting-lr 1e-5 \
    --min-lr 1e-7 \
    --override-scheduling \
    --amp
```

> **Warning:** Without `--override-scheduling`, any new LR flags will be **ignored** when resuming.

### Expanding a Trained Model (`expand_model.py`) 🌱

Create a larger model by **transplanting weights** from a smaller, trained Chronos model directory.

```bash
python expand_model.py \
    --old-model-path "./small_model_dir" \
    --output-dir "./large_model_dir" \
    --context_dim 1024 \
    --h_hidden 1024 \
    --l_hidden 1024 \
    `# Optional: Expand max_length automatically` \
    `# --auto-max-length` \
    `# --dataset-for-length "./larger_dataset.jsonl"`
```

### Continuing Training from an Expanded or Inference Model (`chronos.py`) ✨

Load *only the weights* from an existing model directory (expanded, inference, etc.) using `--model-path` in `train` mode to start a *new* training session (fresh optimizer/scheduler).

```bash
python chronos.py train \
    --pre_pt_dataset `# Example using pre-chunked data` \
    --train "./larger_dataset_chunked_tensors" \
    --max_length 4096 `# MUST specify for pre-chunked` \
    --model-path "./large_model_dir" `# Load weights from expanded model` \
    --tokenizer-path "./large_model_dir" `# Load tokenizer from there too` \
    --out-dir "./large_model_continued_training" \
    --epochs 5 \
    --starting-lr 5e-5 \
    --amp
```

> **Key Difference:**
>
>   * `--resume-from-ckpt`: Loads **full training state** (weights, optimizer, scheduler, epoch) from a `.pt` file to continue an interrupted run.
>   * `--model-path` (in `train` mode): Loads **only model weights** from a directory to start a *new* training run, initializing optimizer/scheduler fresh.

### Chat Mode Features (`chronos.py`)

#### Querying Structured Memory

Use `/filter time=-<seconds>` or `/filter source=<id>` (0=Unknown, 1=User, 2=Training) to constrain memory retrieval. Use `/filter reset` to clear filters.

#### Enabling Online Learning in Chat

Requires the quantized model (`--model-path`) and the original full-precision model (`--shadow-model-path`).

```bash
python chronos.py chat \
    --model-path "./my_model_merged-INT4" \
    --enable-quantized-learning \
    --shadow-model-path "./my_model_merged" \
    --amp # <-- Optionally enable AMP for faster online learning
```

-----

## ⚙️ Command-Line Reference

### `chronos.py` Arguments

| Argument                | Mode(s)                             | Description                                                                                                                               | Default           |
| :---------------------- | :---------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------- | :---------------- |
| **Paths & Data** |                                     |                                                                                                                                           |                   |
| `--model-path`          | `train`, `finetune`, `merge`, `quantize`, `chat` | Path to model directory. **[Train]**: Loads weights only (starts fresh training). **[Other]**: Loads for the specified mode. | `None`            |
| `--train`               | `train`, `finetune`                 | Path to the training `.json`/`.jsonl` file **OR** directory for `--pre_pt_dataset`.                                                            | `None`            |
| `--pre_chunked_dataset` | `train`, `finetune`                 | Load pre-chunked JSONL dataset iteratively (requires `--max_length`). Mutually exclusive with `--pre_pt_dataset`.                            | `False`           |
| `--pre_pt_dataset`      | `train`, `finetune`                 | Load pre-chunked `.pt` tensor dataset from directory specified in `--train` (requires `--max_length`). Mutually exclusive with `--pre_chunked_dataset`. | `False`           |
| `--out-dir`             | `train`, `finetune`, `merge`, `quantize` | Directory to save new models, checkpoints, or adapters.                                                                                   | `./chronos_model` |
| `--tokenizer-path`      | `train`                             | Path or HF name of the tokenizer for a new model (or if overriding model's).                                                              | `microsoft/phi-2` |
| `--resume-from-ckpt`    | `train`                             | Path to `.pt` checkpoint to **resume full training state** (optimizer, etc.).                                                             | `None`            |
| `--shadow-model-path`   | `chat`                              | Path to full-precision model dir for online learning with quantized model.                                                                | `None`            |
| `--lora-adapter-path`   | `merge`, `finetune`                 | Path to the trained LoRA adapter directory.                                                                                               | `None`            |
| **Training/Fine-Tuning**|                                     |                                                                                                                                           |                   |
| `--epochs`              | `train`, `finetune`                 | Number of training epochs.                                                                                                                | `3`               |
| `--batch_size`          | `train`, `finetune`                 | Number of samples per forward pass.                                                                                                       | `4`               |
| `--accumulation-steps`  | `train`, `finetune`                 | Number of steps to accumulate gradients over (simulates larger batch size).                                                               | `1`               |
| `--grad-clip`           | `train`, `finetune`                 | Gradient clipping value. Prevents gradient explosion (0 to disable).                                                                      | `1.0`             |
| `--ponder-loss-weight`  | `train`, `finetune`                 | Weight for the Ponder Cost auxiliary loss.                                                                                                | `0.01`            |
| `--override-scheduling` | `train`                             | **[If resuming]** Ignore checkpoint's schedule state and use new LR args.                                                                 | `False`           |
| `--starting-lr`         | `train`, `finetune`                 | Max Learning Rate for the schedule, or fixed LR if schedule disabled.                                                                     | `1e-4`            |
| `--min-lr`              | `train`, `finetune`                 | Minimum Learning Rate for cosine annealing schedule.                                                                                      | `1e-6`            |
| `--disable-lr-schedule` | `train`, `finetune`                 | Use a fixed Learning Rate (`--starting-lr`) instead of cosine annealing.                                                                  | `False`           |
| `--ltm_lr`              | `train`, `finetune`, `chat`         | Learning Rate for LTM "surprise" updates (or max LR for LTM schedule in chat).                                                            | `0.01`            |
| `--amp`                 | `train`, `finetune`, `chat`         | **Enable Automatic Mixed Precision (requires CUDA).** | `False`           |
| `--num_workers`         | `train`, `finetune`                 | Number of CPU workers for data loading.                                                                                                   | `0`               |
| `--lora_r`              | `finetune`                          | LoRA rank 'r'.                                                                                                                            | `8`               |
| `--lora_alpha`          | `finetune`                          | LoRA alpha scaling factor.                                                                                                                | `16`              |
| `--finetune-unlock-percent` | `finetune`                      | Target % of params to train (approx.). Overrides `--lora_r` if set.                                                                      | `None`            |
| `--kayla`               | `train`, `finetune`                 | Enable Kayla-style instruction tuning format (with thought-process). **Ignored if using pre-chunked formats.** | `False`           |
| **Quantization/Inference**|                                     |                                                                                                                                           |                   |
| `--qtype`               | `quantize`, `train`                 | Quantization format (`INT4`, `Q4_0`, `Q8_0`, `Q2_K`). Used by `quantize` or `--quantize-on-complete`.                                         | `INT4`            |
| `--quantize-on-complete`| `train`                             | Automatically run quantization after training finishes.                                                                                   | `False`           |
| `--device`              | `chat`                              | Device for *quantized* inference (`cpu`, `vulkan`).                                                                                       | `cpu`             |
| `--h-halt-thresh`       | `chat`                              | Probability threshold for early exiting the HRM reasoning loop during inference.                                                          | `0.9`             |
| `--max-new-tokens`      | `chat`                              | Maximum number of tokens to generate in chat mode.                                                                                        | `512`             |
| `--enable-quantized-learning`| `chat`                         | Enable LTM updates for quantized models (requires `--shadow-model-path`).                                                                 | `False`           |
| `--ltm-lora-path`       | `chat`                              | Optional: Path to save/load LTM updates as a separate delta file in chat mode.                                                            | `None`            |
| `--static-ltm-lr`       | `chat`                              | Disable cosine annealing for chat LTM updates, use fixed `--ltm_lr`.                                                                      | `False`           |
| `--ltm-schedule-steps`  | `chat`                              | Number of chat updates per LTM LR cosine cycle.                                                                                           | `100`             |
| `--ltm-schedule-min-lr` | `chat`                              | Minimum LR for chat LTM cosine schedule.                                                                                                  | `1e-5`            |
| **Architecture (Train)**|                                     | *(Used only if starting train from scratch)* |                   |
| `--context_dim`         | `train`                             | Core embedding dimension.                                                                                                                 | `512`             |
| `--persistent_dim`      | `train`                             | Dimension of the fixed Persistent Memory.                                                                                                 | `128`             |
| `--ltm_slots`           | `train`                             | Number of slots in the Long-Term Memory.                                                                                                  | `2048`            |
| `--ltm_key_dim`         | `train`                             | Dimension of LTM keys.                                                                                                                    | `128`             |
| `--ltm_val_dim`         | `train`                             | Dimension of LTM values.                                                                                                                  | `128`             |
| `--h_hidden`            | `train`                             | Hidden size of the High-Level (CEO) RNN.                                                                                                  | `512`             |
| `--l_hidden`            | `train`                             | Hidden size of the Low-Level (Worker) RNN.                                                                                                | `512`             |
| `--max_h_steps`         | `train`                             | Maximum number of reasoning steps the H-module can take per token.                                                                        | `10`              |
| `--max_l_steps`         | `train`                             | Maximum number of iterations for L-module convergence per H-step.                                                                         | `10`              |
| `--l_conv_atol`         | `train`                             | Absolute tolerance for checking L-module state convergence.                                                                               | `1e-5`            |
| `--ltm_topk`            | `train`                             | Number of LTM slots to retrieve per token.                                                                                                | `4`               |
| `--max_length`          | `train`, `finetune`                 | Maximum sequence length. **Required if using `--pre_chunked_dataset` or `--pre_pt_dataset`**.                                             | `1024`            |
| `--auto-max-length`     | `train`, `finetune`                 | Automatically scan `--train` dataset to set `max_length`. **Ignored if using pre-chunked formats.** | `False`           |
| **Other** |                                     |                                                                                                                                           |                   |
| `--threads`             | `All`                               | Number of CPU threads for PyTorch/OpenMP.                                                                                                 | `CPU_Count/2`     |

### `dataset_chunk_create.py` Arguments ✂️

| Argument         | Description                                                               | Required | Default                         |
| :--------------- | :------------------------------------------------------------------------ | :------- | :------------------------------ |
| `--dataset`      | Path to the input **JSONL** dataset file (Kayla format recommended).      | Yes      |                                 |
| `--tokenizer-path`| Path or Hugging Face name of the tokenizer to use for chunking.         | No       | `openai-community/gpt2`         |
| `--output-dir`   | Directory to save the output `.pt` chunk files and `manifest.jsonl`.    | No       | `train_chronos_chunked_tensors` |
| `--overlap`      | Number of tokens to overlap between consecutive chunks.                   | No       | `1024`                          |

### `expand_model.py` Arguments 🌱

| Argument             | Description                                                                       | Required | Default |
| :------------------- | :-------------------------------------------------------------------------------- | :------- | :------ |
| `--old-model-path`   | Path to the trained smaller model *directory*.                                    | Yes      |         |
| `--output-dir`       | Path to save the new, expanded model *directory*.                                 | Yes      |         |
| `--context_dim`      | *Optional:* New context dimension.                                                | No       |         |
| `--persistent_dim`   | *Optional:* New persistent memory dimension.                                      | No       |         |
| `--ltm_slots`        | *Optional:* New number of LTM slots.                                              | No       |         |
| `--ltm_key_dim`      | *Optional:* New LTM key dimension.                                                | No       |         |
| `--ltm_val_dim`      | *Optional:* New LTM value dimension.                                              | No       |         |
| `--h_hidden`         | *Optional:* New H-RNN hidden size.                                                | No       |         |
| `--l_hidden`         | *Optional:* New L-RNN hidden size.                                                | No       |         |
| `--new-max-length`   | *Optional:* Manually specify the new maximum sequence length.                     | No       |         |
| `--auto-max-length`  | *Optional:* Automatically determine `new-max-length` by scanning a dataset.       | No       | `False` |
| `--dataset-for-length`| Path to dataset (.jsonl/.json) required if using `--auto-max-length`.           | If above |         |
| `--kayla`            | Use Kayla formatting when scanning dataset with `--auto-max-length`.            | No       | `False` |

-----

## Roadmap

  - [ ] Develop a user-friendly GUI wrapper for easier interaction.
  - [ ] Extend the architecture to support multi-modal inputs (images, audio).
  - [ ] Implement the entire training loop in Vulkan/CUDA for end-to-end GPU acceleration.

## License

The source code of Chronos is available to the public under a custom license. It is free for non-commercial use, research, and evaluation. However, any commercial use resulting in profit is subject to a profit-sharing agreement. See `LICENSE.md` for full details.

## Support This Project

Please consider supporting my work on Patreon. I have motor cortex damage, which prevents me from working in a traditional tech role. I work on Chronos in my spare time while working full-time at a grocery store.

**[https://www.patreon.com/cw/MakhiBurroughs](https://www.patreon.com/cw/MakhiBurroughs)**

## Acknowledgements

  - This architecture is inspired by the concepts in Google's **Titans** and Sapient Intelligence's **HRM** papers.
  - The quantization kernel design is heavily influenced by the groundbreaking work in **llama.cpp**.
  - **pybind11** for seamless C++/Python integration.

## Changelog

### v0.6 (alpha)

  - **Added Dataset Pre-processing Script (`dataset_chunk_create.py`)**: New tool to chunk large `.jsonl` datasets into fixed-size, overlapping `.pt` tensor files for efficient loading. Handles tokenization, padding, masking, and dynamic sequence length calculation based on dataset analysis (anchoring thought-process in Kayla format).
  - **Implemented Direct Tensor Dataset Loading (`--pre_pt_dataset`)**: `chronos.py` can now load datasets directly from the `.pt` files and manifest generated by the chunking script, dramatically reducing RAM usage and startup time. Requires `--max_length` to be specified, matching the chunker's output.
  - **Implemented Iterable Pre-Chunked JSONL Loading (`--pre_chunked_dataset`)**: Added support for loading pre-processed (tokenized, padded, masked) data from large JSONL files line-by-line using `IterableDataset` for low memory overhead. Requires `--max_length`.
  - **Updated Dataloader Logic**: `train` and `finetune` modes now conditionally select the appropriate dataloader based on `--pre_pt_dataset` or `--pre_chunked_dataset` flags.
  - **Refined Training State Saving**: Checkpoints now save dataset type flags (`pre_pt_dataset`, `pre_chunked_dataset`) to aid resumption.
  - **Documentation**: Updated README extensively to cover the new dataset chunking workflow, command-line arguments, and usage instructions. Added `dataset_chunk_create.py` to the command reference.

### v0.5.2 (alpha)

  - **Added Flexible Training Initiation**: The `train` mode now supports a `--model-path` argument to load *only weights* from an existing model directory (e.g., an expanded model or inference checkpoint) and start a *new* training session (fresh optimizer/scheduler). This is distinct from `--resume-from-ckpt` which loads the full training state.
  - **Enhanced `expand_model.py` Script**:
      - Added ability to expand `max_length` by correctly handling positional embeddings.
      - Added `--new-max-length` flag for manual specification.
      - Added `--auto-max-length` and `--dataset-for-length` flags to automatically detect required `max_length` from a dataset.
      - Automatically copies tokenizer files to the expanded model directory.
  - **Added Automatic Mixed Precision (AMP)**: Implemented `--amp` flag for `train`, `finetune`, and `chat` (online learning) modes to accelerate computation and reduce memory usage on NVIDIA CUDA GPUs.
  - **Documentation**: Updated README with detailed usage for the enhanced `expand_model.py`, AMP, and the new training initiation workflow using `--model-path`. Updated command reference table.

### v0.5.1 (alpha)

  - **Added `--override-scheduling` flag**: Allows users to force new learning rate schedule settings (`--starting-lr`, `--min-lr`) when resuming training from a checkpoint.
  - **Documentation**: Added usage instructions for `--override-scheduling`.

### v0.5 (alpha)

  - **Implemented Structured Long-Term Memory**: Memory slots are now augmented with timestamps and source metadata, enabling temporal and source-based filtering during chat.
  - **Implemented Adaptive Reasoning Depth (Ponder Time)**: The HRM's reasoning depth is now dynamic. The model learns to "think longer" for complex tokens and halt early on simple ones.
  - **Added Ponder Cost**: A new auxiliary loss (`--ponder-loss-weight`) trains the model to be computationally efficient.
  - **Added Halting Threshold**: A new inference flag (`--h-halt-thresh`) allows users to control the trade-off between speed and reasoning depth during chat.

### v0.4 (alpha)

  - **Implemented Dynamic LTM Learning Rate**: Online learning now defaults to a `CosineAnnealingLR` schedule for more stable knowledge consolidation.
  - **Added Static LR Fallback**: The `--static-ltm-lr` flag can be used in chat mode to revert to a fixed LTM learning rate.
  - **Added Gradient Clipping**: The `--grad-clip` argument was added to `train` and `finetune` modes to improve training stability.

-----

© 2025 Makhi Burroughs
